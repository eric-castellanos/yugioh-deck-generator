name: Terraform Destroy (Legacy)

# âš ï¸ DEPRECATED: This workflow is kept for backward compatibility  
# ðŸ‘‰ Use 'infrastructure-destroy.yml' for new deployments
# ðŸ“š See .github/workflows/README.md for migration guide

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: "Type 'destroy' to confirm"
        required: true
      use_existing_resources:
        description: 'Use existing resources configuration'
        required: true
        default: true
        type: boolean

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-east-1

jobs:
  cleanup-kubernetes:
    if: ${{ github.event.inputs.confirm == 'destroy' }}
    runs-on: ubuntu-latest
    env:
      ENVIRONMENT: dev
      AWS_REGION: us-east-1
    
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.27.0'

      - name: Cleanup Kubernetes Resources
        continue-on-error: true
        run: |
          CLUSTER_NAME="mlflow-cluster-${{ env.ENVIRONMENT }}"
          if aws eks describe-cluster --name "$CLUSTER_NAME" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "ðŸ§¹ Cleaning up Kubernetes resources..."
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name "$CLUSTER_NAME"
            
            # Delete MLflow namespace and all resources
            kubectl delete namespace mlflow --ignore-not-found=true --timeout=300s
            
            # Delete any LoadBalancer services
            kubectl delete svc --all-namespaces --field-selector spec.type=LoadBalancer --timeout=300s
            
            echo "âœ… Kubernetes cleanup completed"
          else
            echo "âš ï¸ EKS cluster not found or not accessible, skipping Kubernetes cleanup"
          fi

  terraform-destroy:
    if: ${{ github.event.inputs.confirm == 'destroy' }}
    needs: [cleanup-kubernetes]
    runs-on: ubuntu-latest
    env:
      ENVIRONMENT: dev
      AWS_REGION: us-east-1
      TF_IN_AUTOMATION: true

    defaults:
      run:
        working-directory: infra/terraform

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Create Environment-Specific tfvars
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          if [ "${{ github.event.inputs.use_existing_resources }}" == "true" ]; then
            # Use existing tfvars for existing resources
            cp terraform.tfvars current.tfvars
          else
            # Create tfvars for destruction
            cat > current.tfvars << EOF
          environment        = "${{ env.ENVIRONMENT }}"
          region             = "${{ env.AWS_REGION }}"
          db_password        = "${{ secrets.MLFLOW_DB_PASSWORD }}"
          bucket_name        = "mlflow-backend-${{ env.ENVIRONMENT }}"
          mlflow_user        = "mlflow-user-${{ env.ENVIRONMENT }}"
          existing_resources = false
          account_id         = "${ACCOUNT_ID}"
          github_actions_role_arn = "${{ secrets.AWS_ROLE_TO_ASSUME }}"
          EOF
          fi

      - name: Terraform Init
        run: terraform init -input=false

      - name: Terraform Destroy (initial attempt)
        id: destroy
        continue-on-error: true
        shell: bash
        run: |
          set +e
          set -o pipefail
          RAW_LOG="raw_destroy_output.txt"

          terraform destroy -auto-approve \
            -var-file="current.tfvars" \
            -var "cluster_name=mlflow-cluster-${{ env.ENVIRONMENT }}" \
            > $RAW_LOG 2>&1
          
          #-target=module.eks \
          #-target=module.mlflow \

          DESTROY_EXIT_CODE=$?

          # Extract lock ID
          LOCK_ID=$(grep -Eo 'ID:[[:space:]]+[a-f0-9-]+' "$RAW_LOG" | awk '{print $2}')

          if [[ -n "$LOCK_ID" ]]; then
            echo "Terraform state lock detected: $LOCK_ID"
            echo "lock_id=$LOCK_ID" >> $GITHUB_OUTPUT
          else
            echo "lock_id=" >> $GITHUB_OUTPUT
          fi

          # Export success status
          if [[ $DESTROY_EXIT_CODE -eq 0 ]]; then
            echo "destroy_success=true" >> $GITHUB_OUTPUT
          else
            echo "destroy_success=false" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Force Unlock (if needed)
        if: ${{ steps.destroy.outputs.lock_id != '' }}
        run: terraform force-unlock -force ${{ steps.destroy.outputs.lock_id }}

      - name: Terraform Destroy (retry after unlock)
        if: ${{ steps.destroy.outputs.destroy_success == 'false' }}
        run: |
          terraform destroy -input=false -auto-approve \
            -var-file="current.tfvars" \
            -var "cluster_name=mlflow-cluster-${{ env.ENVIRONMENT }}" \
            # -target=module.eks \
            # -target=module.mlflow

      - name: Upload Destroy Log
        uses: actions/upload-artifact@v4
        with:
          name: destroy-log
          path: infra/terraform/raw_destroy_output.txt


